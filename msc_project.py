# -*- coding: utf-8 -*-
"""MSc Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FkddkT9q2S5UD6_IfwpLGA0FtIAju8DF
"""

# train_icde_fmq.py
# ------------------------------------------------------------
# In-Context Density Estimation with Fully Masked Queries only
# Context is fully visible; queries are all masked.
# The model must learn p(x | context) and is trained via MDN NLL.
# ------------------------------------------------------------
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# -----------------------------
# Config
# -----------------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
DTYPE = torch.float32
EPS = 1e-8


# =================================
# Episodic GMM meta-dataset (with params)
# =================================
class EpisodicGMM(Dataset):
    """
    Each __getitem__ returns one episode:
      - Xc: (Nc, D) context samples (fully visible)
      - Xq: (Nq, D) query samples (used as supervision only)
      - ep_stats: (2D,) [context mean, context std] for per-episode normalization
      - gmm_params: dictionary of true GMM parameters (pis, means, sigmas)
    """
    def __init__(self, n_episodes=20000, dim=2, K_max=5, Nc=16, Nq=64,
                 normalize_episode=True, seed=0):
        self.n_episodes = n_episodes
        self.dim = dim
        self.K_max = K_max
        self.Nc = Nc
        self.Nq = Nq
        self.normalize_episode = normalize_episode
        self.rng = np.random.default_rng(seed)

    def _sample_gmm(self):
        K = self.rng.integers(2, self.K_max + 1)
        pis = self.rng.dirichlet(np.ones(K))
        means = self.rng.normal(0, 5, size=(K, self.dim))
        # diagonal covariances (per-dim std in [0.3, 2.0])
        sigmas = 0.3 + self.rng.random((K, self.dim)) * 1.7
        return K, pis, means.astype(np.float32), sigmas.astype(np.float32)

    def _sample_points(self, K, pis, means, sigmas, N):
        comp = self.rng.choice(K, size=N, p=pis)
        X = means[comp] + self.rng.standard_normal((N, self.dim)) * sigmas[comp]
        return X.astype(np.float32)

    def __len__(self):
        return self.n_episodes

    def __getitem__(self, idx):
        K, pis, means, sigs = self._sample_gmm()
        Xc = self._sample_points(K, pis, means, sigs, self.Nc)
        Xq = self._sample_points(K, pis, means, sigs, self.Nq)

        if self.normalize_episode:
            mu = Xc.mean(axis=0, keepdims=True)
            std = Xc.std(axis=0, keepdims=True) + 1e-6
            Xc = (Xc - mu) / std
            Xq = (Xq - mu) / std
            means = (means - mu) / std
            sigs = sigs / std
            ep_stats = np.concatenate([mu, std], axis=-1).astype(np.float32)  # (1, 2D)
        else:
            ep_stats = np.zeros((1, 2 * self.dim), dtype=np.float32)

        gmm_params = {
            "pis": torch.tensor(pis, dtype=torch.float32),       # (K,)
            "means": torch.tensor(means, dtype=torch.float32),   # (K, D)
            "sigmas": torch.tensor(sigs, dtype=torch.float32)    # (K, D)
        }

        return (
            torch.tensor(Xc, dtype=torch.float32),          # (Nc, D)
            torch.tensor(Xq, dtype=torch.float32),          # (Nq, D)
            torch.tensor(ep_stats.squeeze(0), dtype=torch.float32),  # (2D,)
            gmm_params,
        )

import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs


dataset = EpisodicGMM(n_episodes=1, dim=2, K_max=5, Nc=50, Nq=200)
Xc, Xq, _, _ = dataset[0]  # one episode

# Convert to numpy
Xc = Xc.numpy()
Xq = Xq.numpy()

plt.figure(figsize=(6, 6))
plt.scatter(Xc[:, 0], Xc[:, 1], c='red', marker='o', label="Context (Xc)")
plt.scatter(Xq[:, 0], Xq[:, 1], c='blue', marker='x', alpha=0.6, label="Query (Xq)")
plt.title("Episodic GMM Dataset (Unmasked)")
plt.xlabel("x₁")
plt.ylabel("x₂")
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Take one episode
dataset = EpisodicGMM(n_episodes=1, dim=2, K_max=5, Nc=50, Nq=200)
Xc, Xq, _, _ = dataset[0]

# Convert to numpy
Xc = Xc.numpy()
Xq = Xq.numpy()

fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Left: Unmasked
axes[0].scatter(Xc[:, 0], Xc[:, 1], c='red', marker='o', label="Context (Xc)")
axes[0].scatter(Xq[:, 0], Xq[:, 1], c='blue', marker='x', alpha=0.6, label="Query (Xq)")
axes[0].set_title("Unmasked Episode (Full Data)")
axes[0].set_xlabel("x₁")
axes[0].set_ylabel("x₂")
axes[0].legend()
axes[0].grid(True)

# Right: Masked (queries hidden)
axes[1].scatter(Xc[:, 0], Xc[:, 1], c='red', marker='o', label="Context (Xc)")
# Plot queries as hollow gray circles (masked)
axes[1].scatter(Xq[:, 0], Xq[:, 1], facecolors='none', edgecolors='gray', alpha=0.6, label="Masked Query (Xq)")
axes[1].set_title("Masked Episode (What Model Sees)")
axes[1].set_xlabel("x₁")
axes[1].set_ylabel("x₂")
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

# train_icde_fmq.py
# ------------------------------------------------------------
# In-Context Density Estimation with Fully Masked Queries only
# Context is fully visible; queries are all masked.
# The model must learn p(x | context) and is trained via MDN NLL.
# ------------------------------------------------------------
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, default_collate # Import default_collate

# -----------------------------
# Config
# -----------------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
DTYPE = torch.float32
EPS = 1e-8


# =================================
# Episodic GMM meta-dataset (with params)
# =================================
class EpisodicGMM(Dataset):
    """
    Each __getitem__ returns one episode:
      - Xc: (Nc, D) context samples (fully visible)
      - Xq: (Nq, D) query samples (used as supervision only)
      - ep_stats: (2D,) [context mean, context std] for per-episode normalization
      - gmm_params: dictionary of true GMM parameters (pis, means, sigmas)
    """
    def __init__(self, n_episodes=20000, dim=2, K_max=5, Nc=16, Nq=64,
                 normalize_episode=True, seed=0):
        self.n_episodes = n_episodes
        self.dim = dim
        self.K_max = K_max
        self.Nc = Nc
        self.Nq = Nq
        self.normalize_episode = normalize_episode
        self.rng = np.random.default_rng(seed)

    def _sample_gmm(self):
        K = self.rng.integers(2, self.K_max + 1)
        pis = self.rng.dirichlet(np.ones(K))
        means = self.rng.normal(0, 5, size=(K, self.dim))
        # diagonal covariances (per-dim std in [0.3, 2.0])
        sigmas = 0.3 + self.rng.random((K, self.dim)) * 1.7
        return K, pis, means.astype(np.float32), sigmas.astype(np.float32)

    def _sample_points(self, K, pis, means, sigmas, N):
        comp = self.rng.choice(K, size=N, p=pis)
        X = means[comp] + self.rng.standard_normal((N, self.dim)) * sigmas[comp]
        return X.astype(np.float32)

    def __len__(self):
        return self.n_episodes

    def __getitem__(self, idx):
        K, pis, means, sigs = self._sample_gmm()
        Xc = self._sample_points(K, pis, means, sigs, self.Nc)
        Xq = self._sample_points(K, pis, means, sigs, self.Nq)

        if self.normalize_episode:
            mu = Xc.mean(axis=0, keepdims=True)
            std = Xc.std(axis=0, keepdims=True) + 1e-6
            Xc = (Xc - mu) / std
            Xq = (Xq - mu) / std
            means = (means - mu) / std
            sigs = sigs / std
            ep_stats = np.concatenate([mu, std], axis=-1).astype(np.float32)  # (1, 2D)
        else:
            ep_stats = np.zeros((1, 2 * self.dim), dtype=np.float32)

        gmm_params = {
            "pis": torch.tensor(pis, dtype=torch.float32),       # (K,)
            "means": torch.tensor(means, dtype=torch.float32),   # (K, D)
            "sigmas": torch.tensor(sigs, dtype=torch.float32)    # (K, D)
        }

        return (
            torch.tensor(Xc, dtype=torch.float32),          # (Nc, D)
            torch.tensor(Xq, dtype=torch.float32),          # (Nq, D)
            torch.tensor(ep_stats.squeeze(0), dtype=torch.float32),  # (2D,)
            gmm_params,
        )


# =======================================
# Permutation-invariant Transformer (MDN)
# =======================================
class SetTransformerMDN(nn.Module):
    """
    Inputs are built per token as: [x*m, m, (mu_ctx, std_ctx)].
    - Context tokens: m = 1 (fully visible), x*m = x
    - Query tokens:   m = 0 (fully masked),   x*m = 0
    We concatenate context and query tokens (no positional encoding),
    encode with a Transformer, then predict MDN params for query tokens.
    """
    def __init__(self, dim=2, d_model=192, nhead=8, nlayers=3, K_mdn=5, add_ep_stats=True):
        super().__init__()
        self.dim = dim
        self.d_model = d_model
        self.K = K_mdn
        self.add_ep_stats = add_ep_stats

        in_dim = dim + dim  # [x*m, m]
        if add_ep_stats:
            in_dim += 2 * dim  # [mu_ctx, std_ctx]

        self.input_proj = nn.Linear(in_dim, d_model)
        self.seg_embed = nn.Embedding(2, d_model)  # 0=context, 1=query

        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead,
            dim_feedforward=d_model * 4, batch_first=True,
            dropout=0.1, activation="gelu"
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers)

        self.query_proj = nn.Linear(d_model, d_model)
        self.pi_head = nn.Linear(d_model, self.K)
        self.mu_head = nn.Linear(d_model, self.K * dim)
        self.logsig_head = nn.Linear(d_model, self.K * dim)

    def _build_tokens(self, X, M, seg_id, ep_stats):
        # X, M: (B, N, D) ; ep_stats: (B, 2D) or None
        feats = [X * M, M]
        if self.add_ep_stats and ep_stats is not None:
            ep = ep_stats.unsqueeze(1).expand(X.size(0), X.size(1), ep_stats.size(-1))
            feats.append(ep)
        Z = torch.cat(feats, dim=-1)  # (B, N, in_dim)
        H = self.input_proj(Z) + self.seg_embed.weight[seg_id]
        return H

    def forward(self, Xc, Xq, ep_stats):
        """
        Xc: (B, Nc, D) context (fully visible)
        Xq: (B, Nq, D) query values (only used to create query tokens; they are fully masked)
        ep_stats: (B, 2D)
        """
        B, Nc, D = Xc.shape
        Nq = Xq.shape[1]

        # Context masks = 1, Query masks = 0 (fully masked queries)
        Mc = torch.ones(B, Nc, D, device=Xc.device, dtype=Xc.dtype)
        Mq = torch.zeros(B, Nq, D, device=Xq.device, dtype=Xq.dtype)

        tok_c = self._build_tokens(Xc, Mc, seg_id=0, ep_stats=ep_stats)   # (B, Nc, d)
        tok_q = self._build_tokens(torch.zeros_like(Xq), Mq, seg_id=1, ep_stats=ep_stats)  # zeros, m=0

        # Concatenate (keep order: [context | queries])
        tokens = torch.cat([tok_c, tok_q], dim=1)  # (B, Nc+Nq, d)

        # Encode
        H = self.encoder(tokens)
        Hq = H[:, Nc:, :]                           # (B, Nq, d) just the query slice
        Hq = F.gelu(self.query_proj(Hq))

        pi_logits = self.pi_head(Hq)                         # (B, Nq, K)
        mu = self.mu_head(Hq).view(B, Nq, self.K, D)         # (B, Nq, K, D)
        logsig = self.logsig_head(Hq).view(B, Nq, self.K, D) # (B, Nq, K, D)
        sig = torch.exp(logsig).clamp_min(1e-4)
        return pi_logits, mu, sig


# ===============================
# MDN Negative Log-Likelihood
# ===============================
def mdn_nll_gaussian(pi_logits, mu, sig, y):
    """
    pi_logits: (B, Nq, K)
    mu, sig:   (B, Nq, K, D)
    y:         (B, Nq, D)
    """
    log_pi = torch.log_softmax(pi_logits, dim=-1)                # (B, Nq, K)
    y = y.unsqueeze(2)                                           # (B, Nq, 1, D)
    var = sig ** 2
    diff = (y - mu)
    comp_logprob = -0.5 * (
        (diff**2 / var).sum(-1) + torch.log(2 * math.pi * var + EPS).sum(-1)
    )                                                            # (B, Nq, K)
    log_mix = torch.logsumexp(log_pi + comp_logprob, dim=-1)     # (B, Nq)
    return -(log_mix).mean()


# === Custom collate function for training ===
# This collate function will ignore the gmm_params dictionary
def train_collate_fn(batch):
    # The batch will contain tuples of (Xc, Xq, ep_stats, gmm_params)
    # We only want to collate the first three elements for training
    return default_collate([item[:3] for item in batch])


# =========================
# Training (Fully Masked Q)
# =========================
def train_icde_fmq(
    epochs=40,
    batch_size=32,
    Nc=16, Nq=64, dim=2,
    lr=1e-3, weight_decay=1e-4,
    K_mdn=5,
    entropy_bonus=1e-3,
    seed=0,
):
    # Data
    train_set = EpisodicGMM(n_episodes=20000, dim=dim, Nc=Nc, Nq=Nq, seed=seed)
    val_set   = EpisodicGMM(n_episodes=1000,  dim=dim, Nc=Nc, Nq=Nq, seed=seed + 1)

    train_loader = DataLoader(
        train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True,
        collate_fn=train_collate_fn # Use the custom collate function
    )
    val_loader = DataLoader(
        val_set, batch_size=batch_size, shuffle=False, pin_memory=True,
        collate_fn=train_collate_fn # Use the custom collate function for validation too
    )

    # Model / Optim
    model = SetTransformerMDN(
        dim=dim, d_model=128, nhead=8, nlayers=3, K_mdn=K_mdn, add_ep_stats=True
    ).to(DEVICE)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    best_val = float("inf")

    for ep in range(1, epochs + 1):
        # ------------- Train -------------
        model.train()
        tr_loss = 0.0
        # The train_collate_fn returns only Xc, Xq, ep_stats
        for Xc, Xq, ep_stats in train_loader:
            Xc, Xq, ep_stats = Xc.to(DEVICE), Xq.to(DEVICE), ep_stats.to(DEVICE)

            # Forward: queries are fully masked inside the model
            pi_logits, mu, sig = model(Xc, Xq, ep_stats)

            # NLL on true query targets (teacher forcing)
            nll = mdn_nll_gaussian(pi_logits, mu, sig, Xq)

            # small entropy bonus on sigmas to avoid variance collapse
            ent = 0.5 * torch.log(sig**2 + EPS).mean()
            loss = nll - entropy_bonus * ent

            opt.zero_grad()
            loss.backward()
            opt.step()

            tr_loss += loss.item() * Xc.size(0)

        tr_loss /= len(train_loader.dataset)

        # ------------- Validation -------------
        model.eval()
        with torch.no_grad():
            val_loss = 0.0
            # The train_collate_fn returns only Xc, Xq, ep_stats
            for Xc, Xq, ep_stats in val_loader:
                Xc, Xq, ep_stats = Xc.to(DEVICE), Xq.to(DEVICE), ep_stats.to(DEVICE)
                pi_logits, mu, sig = model(Xc, Xq, ep_stats)
                nll = mdn_nll_gaussian(pi_logits, mu, sig, Xq)
                ent = 0.5 * torch.log(sig**2 + EPS).mean()
                loss = nll - entropy_bonus * ent
                val_loss += loss.item() * Xc.size(0)
            val_loss /= len(val_loader.dataset)

        print(f"Epoch {ep:03d} | Train Obj: {tr_loss:.4f} | Val Obj: {val_loss:.4f}")

        if val_loss < best_val - 1e-6:
            best_val = val_loss
            torch.save(model.state_dict(), "icde_fmq_best.pt")

    print(f"✅ Best Val Objective: {best_val:.4f}")
    return model


# -----------------------------
# Run
# -----------------------------
if __name__ == "__main__":
    _ = train_icde_fmq(
        epochs=40,
        batch_size=32,
        Nc=16, Nq=64, dim=2,
        lr=1e-3, weight_decay=1e-4,
        K_mdn=5,
        entropy_bonus=1e-3,
        seed=0,
    )

import torch
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader, default_collate
from scipy.stats import multivariate_normal, gaussian_kde
import math

# === Utility: compute analytic GMM density ===
def compute_gmm_density(grid_points, weights, means, covs):
    """Compute GMM density at grid points (2D)."""
    density = np.zeros(len(grid_points))
    for w, mu, cov in zip(weights, means, covs):
        rv = multivariate_normal(mean=mu, cov=cov)
        density += w * rv.pdf(grid_points)
    return density

# === Utility: compute contour levels for probability mass ===
def get_density_levels(Z, levels=[0.1, 0.3, 0.5, 0.9]):
    """Return contour thresholds for specified probability mass levels."""
    Z_flat = Z.ravel()
    Z_sorted = np.sort(Z_flat)[::-1]
    cumsum = np.cumsum(Z_sorted)
    cumsum /= (cumsum[-1] + 1e-8) # Adding small epsilon to avoid division by zero

    thresholds = []
    for p in levels:
        idx = np.searchsorted(cumsum, p)
        if idx < len(Z_sorted):
            thresholds.append(Z_sorted[idx])
        else:
            thresholds.append(Z_sorted[-1]) # Use the minimum density if probability mass exceeds max

    # Ensure levels are unique and increasing
    thresholds = sorted(list(set(thresholds)))
    return thresholds

# === Custom collate function to handle gmm_params ===
def custom_collate_fn(batch):
    Xc_batch = [item[0] for item in batch]
    Xq_batch = [item[1] for item in batch]
    ep_stats_batch = [item[2] for item in batch]
    gmm_params_batch = [item[3] for item in batch]

    collated_data = default_collate([
        (Xc, Xq, ep_stats) for Xc, Xq, ep_stats, _ in batch
    ])

    return collated_data[0], collated_data[1], collated_data[2], gmm_params_batch


@torch.no_grad()
def evaluate_icde_fmq(
    model_path,
    dataloader,
    device="cuda",
    n_batches=5,
    save_prefix="eval",
    save_format="png"
):
    # === Load trained model ===
    model = SetTransformerMDN(dim=2, d_model=128, nhead=8, nlayers=3,
                              K_mdn=5, add_ep_stats=True).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    all_scatter_data, all_contour_data, all_hist_data = [], [], []

    for batch_idx, (Xc, Xq, ep_stats, gmm_params_batch) in enumerate(dataloader):
        if batch_idx >= n_batches:
            break
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        B, Nc, D = Xc.shape
        Nq = Xq.shape[1]
        K = model.K

        # Context fully visible, queries masked
        Mc = torch.ones(B, Nc, D, device=device, dtype=Xc.dtype)
        Mq = torch.zeros(B, Nq, D, device=device, dtype=Xq.dtype)
        Xq_masked = Xq * Mq

        # Forward pass
        pi_logits, mu, sig = model(Xc, Xq_masked, ep_stats)

        # De-normalize
        mu_ctx = ep_stats[:, :D].unsqueeze(1)
        std_ctx = ep_stats[:, D:].unsqueeze(1)
        Xq_denorm = Xq * std_ctx + mu_ctx

        for b in range(min(2, B)):
            true_xy = Xq_denorm[b].cpu().numpy()

            # Ground truth GMM (from dataset)
            gmm_params = gmm_params_batch[b]
            pis_true = gmm_params["pis"].cpu().numpy()
            means_true = gmm_params["means"].cpu().numpy()
            sigmas_true = gmm_params["sigmas"].cpu().numpy()
            means_true = means_true * std_ctx[b, 0].cpu().numpy() + mu_ctx[b, 0].cpu().numpy()
            sigmas_true = sigmas_true * std_ctx[b, 0].cpu().numpy()
            covs_true = [np.diag(s**2) for s in sigmas_true]

                        # Predicted MDN parameters
            pi_pred = torch.softmax(pi_logits[b], dim=-1).mean(dim=0).cpu().numpy()
            mu_pred = mu[b].mean(dim=0).cpu().numpy()
            sig_pred = sig[b].mean(dim=0).cpu().numpy()
            covs_pred = [np.diag(s**2) for s in sig_pred]

            # Save for plotting
            all_scatter_data.append((true_xy, mu_pred, f"Episode {batch_idx}, Batch {b} (Scatter)"))
            all_contour_data.append((true_xy, pis_true, means_true, covs_true,
                                     pi_pred, mu_pred, covs_pred,
                                     f"Episode {batch_idx}, Batch {b} (Contours)"))
            all_hist_data.append((true_xy, pi_pred, mu_pred, sig_pred,
                                  mu_ctx[b, 0].cpu().numpy(), std_ctx[b, 0].cpu().numpy(),
                                  f"Episode {batch_idx}, Batch {b} (Histograms)"))

    # === Scatter plots ===
    if all_scatter_data:
        ncols = min(5, len(all_scatter_data))
        nrows = math.ceil(len(all_scatter_data) / ncols)
        fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 5, nrows * 5))
        axes = axes.flatten()
        for i, (true_xy, mu_pred, title) in enumerate(all_scatter_data):
            ax = axes[i]
            ax.scatter(true_xy[:, 0], true_xy[:, 1], alpha=0.6, label="True")
            ax.scatter(mu_pred[:, 0], mu_pred[:, 1], alpha=0.6, label="Pred Means")
            ax.legend(); ax.set_title(title)
        for j in range(i+1, len(axes)):
            fig.delaxes(axes[j])
        plt.tight_layout()
        fig.savefig(f"{save_prefix}_scatter.{save_format}", bbox_inches="tight")
        plt.close(fig)

    # === Contour plots ===
    if all_contour_data:
        ncols = min(5, len(all_contour_data))
        nrows = math.ceil(len(all_contour_data) / ncols)
        fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 5, nrows * 5))
        axes = axes.flatten()
        for i, (true_xy, pis_true, means_true, covs_true,
                pi_pred, mu_pred, covs_pred, title) in enumerate(all_contour_data):
            ax = axes[i]

            # Grid
            mins, maxs = true_xy.min(axis=0)-1.0, true_xy.max(axis=0)+1.0
            xs, ys = np.linspace(mins[0], maxs[0], 200), np.linspace(mins[1], maxs[1], 200)
            Xg, Yg = np.meshgrid(xs, ys)
            grid = np.column_stack([Xg.ravel(), Yg.ravel()])

            # Densities
            Zt = compute_gmm_density(grid, pis_true, means_true, covs_true).reshape(Xg.shape)
            Zp = compute_gmm_density(grid, pi_pred, mu_pred, covs_pred).reshape(Xg.shape)

            # Levels (probability mass contours)
            levels_t = get_density_levels(Zt)
            levels_p = get_density_levels(Zp)

            # Plot samples
            ax.scatter(true_xy[:, 0], true_xy[:, 1], s=10, c="black", alpha=0.4, label="Samples")

            cs1 = ax.contour(Xg, Yg, Zt, levels=levels_t, linewidths=1.4, cmap="Blues")
            cs2 = ax.contour(Xg, Yg, Zp, levels=levels_p, linestyles="--", linewidths=1.4, cmap="Reds")
            ax.set_title(title); ax.set_xlabel("X"); ax.set_ylabel("Y"); ax.legend()
        for j in range(i+1, len(axes)):
            fig.delaxes(axes[j])
        plt.tight_layout()
        fig.savefig(f"{save_prefix}_contours.{save_format}", bbox_inches="tight")
        plt.close(fig)

    # === Histograms with KDE ===
    if all_hist_data:
        ncols = min(5, len(all_hist_data))
        nrows = math.ceil(len(all_hist_data) / ncols)
        D = 2
        fig, axes = plt.subplots(nrows*D, ncols, figsize=(ncols * 5, nrows * D * 3))
        axes = axes.flatten()
        for i, (true_xy, pi_pred, mu_pred, sig_pred, mu_ctx, std_ctx, title) in enumerate(all_hist_data):
            # Sample from predicted MDN
            comp = torch.distributions.Independent(torch.distributions.Normal(
                torch.tensor(mu_pred), torch.tensor(sig_pred)), 1)
            mix = torch.distributions.Categorical(probs=torch.tensor(pi_pred))
            mdn = torch.distributions.MixtureSameFamily(mix, comp)
            pred_xy = mdn.sample((true_xy.shape[0],)).numpy()
            pred_xy = pred_xy * std_ctx + mu_ctx

            for d in range(D):
                ax = axes[i*D+d]
                ax.hist(true_xy[:, d], bins=30, alpha=0.5, density=True, label="True")
                ax.hist(pred_xy[:, d], bins=30, alpha=0.5, density=True, label="Pred")

                kde_true = gaussian_kde(true_xy[:, d])
                kde_pred = gaussian_kde(pred_xy[:, d])
                xs = np.linspace(min(true_xy[:, d].min(), pred_xy[:, d].min())-1,
                                 max(true_xy[:, d].max(), pred_xy[:, d].max())+1, 200)
                ax.plot(xs, kde_true(xs), "b-", label="True KDE")
                ax.plot(xs, kde_pred(xs), "r--", label="Pred KDE")

                ax.set_title(f"{title} - Dim {['X','Y'][d]}")
                ax.legend()
        for j in range(i*D+d+1, len(axes)):
            fig.delaxes(axes[j])
        plt.tight_layout()
        fig.savefig(f"{save_prefix}_histograms.{save_format}", bbox_inches="tight")
        plt.close(fig)

    print("✅ Evaluation complete: scatter, contour, and histogram plots saved.")

from torch.utils.data import DataLoader, default_collate
import torch

def collate_with_gmmparams(batch):
    """
    Custom collate function for EpisodicGMM dataset.
    Keeps gmm_params as a list instead of stacking (since K varies per episode).
    """
    Xc, Xq, ep_stats, gmm_params = zip(*batch)
    return (
        torch.stack(Xc, dim=0),       # (B, Nc, D)
        torch.stack(Xq, dim=0),       # (B, Nq, D)
        torch.stack(ep_stats, dim=0), # (B, 2D)
        list(gmm_params),             # list of dicts, one per episode
    )

# === Calling function ===
if __name__ == "__main__":
    # Build test dataset
    test_dataset = EpisodicGMM(
        n_episodes=100,   # number of episodes
        dim=2,            # 2D GMMs
        Nc=16,            # context size
        Nq=64,            # query size
        seed=42
    )

    # Dataloader with custom collate (to keep gmm_params)
    test_loader = DataLoader(
        test_dataset,
        batch_size=16,
        shuffle=False, # No shuffling for evaluation
        collate_fn=collate_with_gmmparams # Use the custom collate function
    )

    # Run evaluation
    evaluate_icde_fmq(
        model_path="icde_fmq_best.pt",  # trained model checkpoint
        dataloader=test_loader,
        device="cuda",                  # or "cpu"
        n_batches=5,                    # number of batches to visualize
        save_prefix="eval",             # prefix for saved plots
        save_format="png"
    )

import torch
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import wasserstein_distance
from typing import Optional, List, Dict
import csv
from sklearn.mixture import GaussianMixture
from dataclasses import dataclass

# === Wasserstein distance for multi-dim arrays ===
def wasserstein_distance_nd(u, v):
    return np.mean([wasserstein_distance(u[:, d], v[:, d]) for d in range(u.shape[1])])

# === MMD (with Gaussian kernel) ===
def compute_mmd(x, y, sigma=1.0):
    xx = np.exp(-np.square(np.linalg.norm(x[:, None, :] - x[None, :, :], axis=-1)) / (2 * sigma**2)).mean()
    yy = np.exp(-np.square(np.linalg.norm(y[:, None, :] - y[None, :, :], axis=-1)) / (2 * sigma**2)).mean()
    xy = np.exp(-np.square(np.linalg.norm(x[:, None, :] - y[None, :, :], axis=-1)) / (2 * sigma**2)).mean()
    return xx + yy - 2 * xy

# === Per-episode GMM fitting (BIC) ===
def fit_ep_gmm(X, k_min=1, k_max=6, seed=0):
    best_bic = np.inf
    best_gmm = None
    for k in range(k_min, k_max + 1):
        gmm = GaussianMixture(n_components=k, random_state=seed, reg_covar=1e-6)
        gmm.fit(X)
        bic = gmm.bic(X)
        if bic < best_bic:
            best_bic = bic
            best_gmm = gmm
    return best_gmm

# === Fit global Gaussian ===
def fit_global_gaussian(list_of_arrays):
    all_data = np.concatenate(list_of_arrays, axis=0)
    mu = np.mean(all_data, axis=0)
    cov = np.cov(all_data, rowvar=False)
    return mu, cov

def sample_gaussian(mu, cov, n, rng):
    return rng.multivariate_normal(mu, cov, size=n)

# === Metrics container ===
@dataclass
class JointMetrics:
    wd: float
    mmd: float

# === Custom collate function for DataLoader ===
from torch.utils.data import DataLoader

def collate_with_gmmparams(batch):
    Xc, Xq, ep_stats, gmm_params = zip(*batch)
    return (
        torch.stack(Xc, dim=0),       # (B, Nc, D)
        torch.stack(Xq, dim=0),       # (B, Nq, D)
        torch.stack(ep_stats, dim=0), # (B, 2D)
        list(gmm_params),             # list of dicts
    )

# === Evaluate model with bounds ===
@torch.no_grad()
def evaluate_with_bounds(
    model,
    dataloader,
    device="cuda",
    n_batches=5,
    seed: int = 0,
    save_csv: Optional[str] = None,
    upper_kmax: int = 6,
    plot: bool = True,
    save_plot: Optional[str] = None
):
    rng = np.random.default_rng(seed)

    # ---- Pass 1: fit global Gaussian ----
    all_true, cache_batches, taken = [], [], 0
    for batch_idx, (Xc, Xq, ep_stats, _) in enumerate(dataloader):
        if taken >= n_batches: break
        cache_batches.append((Xc, Xq, ep_stats))
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        D = Xc.shape[-1]
        mu_ctx  = ep_stats[:, :D].unsqueeze(1)
        std_ctx = ep_stats[:, D:].unsqueeze(1)
        Xq_denorm = (Xq * std_ctx + mu_ctx).cpu().numpy()
        for b in range(Xc.shape[0]):
            all_true.append(Xq_denorm[b])
        taken += 1

    mu_glob, cov_glob = fit_global_gaussian(all_true)

    # ---- Pass 2: per-episode metrics ----
    per_episode: List[Dict[str, float]] = []
    for (Xc, Xq, ep_stats) in cache_batches:
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        B, Nc, D = Xc.shape
        Nq = Xq.shape[1]

        pi_logits, mu, sig = model(Xc, Xq, ep_stats)
        K = mu.shape[2]

        mu_rs = mu.view(B * Nq, K, D)
        sig_rs = sig.view(B * Nq, K, D)
        comp = torch.distributions.Independent(torch.distributions.Normal(mu_rs, sig_rs), 1)
        mix = torch.distributions.Categorical(logits=pi_logits.view(B * Nq, K))
        gmm = torch.distributions.MixtureSameFamily(mix, comp)
        samples = gmm.sample().view(B, Nq, D)

        mu_ctx  = ep_stats[:, :D].unsqueeze(1)
        std_ctx = ep_stats[:, D:].unsqueeze(1)
        Xq_denorm = (Xq * std_ctx + mu_ctx).cpu().numpy()
        Ypred_denorm = (samples * std_ctx + mu_ctx).cpu().numpy()

        for b in range(B):
            gt = Xq_denorm[b]
            yp = Ypred_denorm[b]

            # lower bound
            low_samples = sample_gaussian(mu_glob, cov_glob, n=len(gt), rng=rng)

            # upper bound: per-episode GMM
            gmm_ep = fit_ep_gmm(gt, k_min=1, k_max=upper_kmax, seed=seed)
            up_samples = gmm_ep.sample(n_samples=len(gt))[0]

            # collect metrics
            wd_ours = wasserstein_distance_nd(gt, yp)
            mmd_ours = compute_mmd(gt, yp)

            wd_low = wasserstein_distance_nd(gt, low_samples)
            mmd_low = compute_mmd(gt, low_samples)

            wd_up = wasserstein_distance_nd(gt, up_samples)
            mmd_up = compute_mmd(gt, up_samples)

            per_episode.append({
                "wd_ours": wd_ours, "mmd_ours": mmd_ours,
                "wd_lower": wd_low,   "mmd_lower": mmd_low,
                "wd_upper": wd_up,    "mmd_upper": mmd_up,
            })

    # ---- Aggregate results ----
    def agg(key):
        vals = np.array([row[key] for row in per_episode], dtype=float)
        return float(vals.mean()), float(vals.std())

    keys = ["wd_ours","mmd_ours",
            "wd_lower","mmd_lower",
            "wd_upper","mmd_upper"]

    print("=== Per-episode metrics (first 5 rows) ===")
    for row in per_episode[:5]:
        print({k: round(row[k], 4) for k in keys})

    print("\n=== Aggregate (mean ± std) ===")
    for label in ["ours","lower","upper"]:
        wd_m, wd_s = agg(f"wd_{label}")
        mmd_m, mmd_s = agg(f"mmd_{label}")
        print(f"[{label:6}] WD: {wd_m:.4f} ± {wd_s:.4f} | MMD: {mmd_m:.4f} ± {mmd_s:.4f}")

    if save_csv is not None:
        with open(save_csv, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            for row in per_episode:
                writer.writerow(row)
        print(f"\nSaved per-episode metrics to {save_csv}")

    if plot:
        episodes = np.arange(len(per_episode))
        fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)
        metrics = ["wd", "mmd"]
        for ax, m in zip(axes, metrics):
            ax.plot(episodes, [row[f"{m}_ours"] for row in per_episode], label="Ours", marker="o")
            ax.plot(episodes, [row[f"{m}_lower"] for row in per_episode], label="Lower bound", marker="x")
            ax.plot(episodes, [row[f"{m}_upper"] for row in per_episode], label="Upper bound", marker="^")
            ax.set_ylabel(m.upper())
            ax.legend()
            ax.grid(True, linestyle="--", alpha=0.6)
        axes[-1].set_xlabel("Episode index")
        plt.tight_layout()
        if save_plot:
            plt.savefig(save_plot, dpi=150)
            print(f"Saved plot to {save_plot}")
        else:
            plt.show()

    return per_episode

# === Run evaluation ===
if __name__ == "__main__":
    # Load dataset
    test_dataset = EpisodicGMM(n_episodes=200, dim=2, Nc=16, Nq=64, seed=42)
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_with_gmmparams)

    # Load trained model
    model = SetTransformerMDN(dim=2, d_model=128, nhead=8, nlayers=3, K_mdn=5, add_ep_stats=True).to(DEVICE)
    model.load_state_dict(torch.load("icde_fmq_best.pt", map_location=DEVICE))
    model.eval()

    # Run evaluation
    results = evaluate_with_bounds(model=model, dataloader=test_loader, device=DEVICE, n_batches=10, seed=0, plot=True, save_plot="per_episode_metrics.png")
    print(f"\nEvaluated {len(results)} episodes")

import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Optional, List, Dict
from sklearn.mixture import GaussianMixture
from scipy.stats import wasserstein_distance
from scipy.stats import multivariate_normal
from scipy.special import logsumexp
import csv
from dataclasses import dataclass

# ================================
# Helper Functions
# ================================

def fit_ep_gmm(X, k_min=1, k_max=6, seed=0):
    """Fit per-episode GMM using BIC selection."""
    best_bic, best_gmm = np.inf, None
    for k in range(k_min, k_max + 1):
        gmm = GaussianMixture(
            n_components=k,
            covariance_type="full",
            reg_covar=1e-6,
            random_state=seed
        )
        gmm.fit(X)
        bic = gmm.bic(X)
        if bic < best_bic:
            best_bic, best_gmm = bic, gmm
    return best_gmm

def fit_global_gaussian(list_of_arrays):
    """Fit a global Gaussian over all episodes."""
    all_data = np.concatenate(list_of_arrays, axis=0)
    mu = np.mean(all_data, axis=0)
    cov = np.cov(all_data, rowvar=False)
    return mu, cov

def sample_gaussian(mu, cov, n, rng):
    return rng.multivariate_normal(mu, cov, size=n)

def sample_true_gmm(pis, means, sigmas, n_samples, rng):
    """Sample from a GMM with diagonal covariance."""
    K = len(pis)
    D = means.shape[-1]
    comp_choices = rng.choice(K, size=n_samples, p=pis)
    samples = means[comp_choices] + rng.standard_normal((n_samples, D)) * sigmas[comp_choices]
    return samples

def mdn_log_prob_numpy(pi_logits_np, mu_np, sig_np, x_np, eps=1e-12):
    """Compute MDN log probability for NumPy arrays."""
    N, D = x_np.shape
    Nq, K, _ = mu_np.shape
    log_pi = pi_logits_np - logsumexp(pi_logits_np, axis=-1, keepdims=True)
    log_probs_per_query = []
    for i in range(Nq):
        mu_i, sig_i, log_pi_i = mu_np[i], sig_np[i], log_pi[i]
        x_exp = x_np[:, None, :]
        mu_exp = mu_i[None, :, :]
        sig_exp = sig_i[None, :, :]
        var = sig_exp**2
        diff = x_exp - mu_exp
        comp_term = -0.5 * (np.sum(diff**2 / (var + eps), axis=-1) + np.sum(np.log(2*np.pi*var + eps), axis=-1))
        log_mix = logsumexp(log_pi_i[None, :] + comp_term, axis=-1)
        log_probs_per_query.append(log_mix)
    log_probs_all = np.stack(log_probs_per_query, axis=1)
    return log_probs_all.mean(axis=1)

def true_gmm_log_prob(pis, means, sigmas, x, eps=1e-12):
    """Compute log-probability of samples under true GMM with diagonal covariances."""
    pis = np.asarray(pis)
    means = np.asarray(means)
    sigmas = np.asarray(sigmas)
    N, D = x.shape
    K = len(pis)
    log_pis = np.log(pis + eps)
    var = sigmas**2
    diff = x[:, None, :] - means[None, :, :]
    quad = np.sum(diff**2 / (var[None, :, :] + eps), axis=-1)
    log_det = np.sum(np.log(2*np.pi*var + eps), axis=-1)
    comp = -0.5 * (quad + log_det[None, :])
    return logsumexp(log_pis[None, :] + comp, axis=-1)

def mc_kl_from_samples(samples, logprob_p, logprob_q):
    """Monte Carlo estimate of KL(P||Q)."""
    lp = logprob_p(samples)
    lq = logprob_q(samples)
    return float(np.mean(lp - lq))

# ================================
# KL Evaluation Function
# ================================

@torch.no_grad()
def evaluate_kl_bounds(
    model,
    dataloader,
    device="cuda",
    n_batches=5,
    n_mdn_samples=500,
    seed: int = 0,
    upper_kmax: int = 6,
    plot: bool = True,
    save_plot: Optional[str] = None,
    save_csv: Optional[str] = None
):
    rng = np.random.default_rng(seed)
    all_true = []
    cache_batches = []
    taken = 0

    # ---- Pass 1: Collect denormalized true data ----
    for batch_idx, (Xc, Xq, ep_stats, gmm_params_batch) in enumerate(dataloader):
        if taken >= n_batches:
            break
        cache_batches.append((Xc, Xq, ep_stats, gmm_params_batch))
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        D = Xc.shape[-1]
        mu_ctx  = ep_stats[:, :D].unsqueeze(1)
        std_ctx = ep_stats[:, D:].unsqueeze(1)
        Xq_denorm = (Xq * std_ctx + mu_ctx).cpu().numpy()
        for b in range(Xc.shape[0]):
            all_true.append(Xq_denorm[b])
        taken += 1

    mu_glob, cov_glob = fit_global_gaussian(all_true)

    # ---- Pass 2: Evaluate KL per episode ----
    per_episode: List[Dict[str, float]] = []
    taken = 0

    for (Xc, Xq, ep_stats, gmm_params_batch) in cache_batches:
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        B, Nc, D = Xc.shape
        Nq = Xq.shape[1]
        pi_logits, mu, sig = model(Xc, Xq, ep_stats)

        for b in range(B):
            # Denormalize data and MDN
            Xq_denorm_b = (Xq[b] * ep_stats[b, D:] + ep_stats[b, :D]).cpu().numpy()
            mu_denorm_b = (mu[b] * ep_stats[b, D:].reshape(1,1,D) + ep_stats[b, :D].reshape(1,1,D)).cpu().numpy()
            sig_denorm_b = (sig[b] * ep_stats[b, D:].reshape(1,1,D)).cpu().numpy()
            pi_logits_np_b = pi_logits[b].cpu().numpy()

            # Sample from predicted MDN (average over query points)
            pi_avg = np.mean(torch.softmax(pi_logits[b], dim=-1).cpu().numpy(), axis=0)[None,:]
            mu_avg = mu_denorm_b.mean(axis=0)[None, :, :]
            sig_avg = sig_denorm_b.mean(axis=0)[None, :, :]
            pred_samples = []
            for _ in range(n_mdn_samples):
                k_choice = rng.choice(mu_avg.shape[1], p=pi_avg[0])
                sample = rng.normal(mu_avg[0,k_choice], sig_avg[0,k_choice])
                pred_samples.append(sample)
            pred_samples = np.stack(pred_samples, axis=0)

            # Ground truth GMM
            gmm_params_b = gmm_params_batch[b]
            pis_true_b = gmm_params_b["pis"].cpu().numpy()
            means_true_b = (gmm_params_b["means"].cpu().numpy() * ep_stats[b, D:].cpu().numpy() + ep_stats[b, :D].cpu().numpy())
            sigmas_true_b = gmm_params_b["sigmas"].cpu().numpy() * ep_stats[b, D:].cpu().numpy()
            true_samples = sample_true_gmm(pis_true_b, means_true_b, sigmas_true_b, n_mdn_samples, rng)

            # KL: True || Predicted
            logprob_true = lambda x: true_gmm_log_prob(pis_true_b, means_true_b, sigmas_true_b, x)
            logprob_pred = lambda x: mdn_log_prob_numpy(pi_avg, mu_avg, sig_avg, x)
            kl_ours = mc_kl_from_samples(true_samples, logprob_true, logprob_pred)

            # KL: True || Lower (global Gaussian)
            logprob_lower = lambda x: multivariate_normal.logpdf(x, mean=mu_glob, cov=cov_glob, allow_singular=True)
            kl_lower = mc_kl_from_samples(true_samples, logprob_true, logprob_lower)

            # KL: True || Upper (per-episode GMM)
            gmm_ep_b = fit_ep_gmm(Xq_denorm_b, k_min=1, k_max=upper_kmax, seed=seed)
            logprob_upper = lambda x: gmm_ep_b.score_samples(x)
            kl_upper = mc_kl_from_samples(true_samples, logprob_true, logprob_upper)

            per_episode.append({
                "kl_ours": kl_ours,
                "kl_lower": kl_lower,
                "kl_upper": kl_upper
            })

        taken += 1
        if taken >= n_batches:
            break

    # ---- Aggregate results ----
    def agg(key):
        vals = np.array([row[key] for row in per_episode], dtype=float)
        return float(vals.mean()), float(vals.std())

    # Print per-episode metrics
    keys = ["kl_ours", "kl_lower", "kl_upper"]
    print("=== Per-episode KL metrics (first 5 rows) ===")
    for row in per_episode[:5]:
        print({k: round(row[k],4) for k in keys})
    print("\n=== Aggregate KL (mean ± std) ===")
    for label in ["ours", "lower", "upper"]:
        kl_m, kl_s = agg(f"kl_{label}")
        print(f"[{label:6}] KL: {kl_m:.4f} ± {kl_s:.4f}")

    # Save CSV if requested
    if save_csv is not None:
        with open(save_csv, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            for row in per_episode:
                writer.writerow(row)
        print(f"\nSaved per-episode KL metrics to {save_csv}")

    # Plot if requested
    if plot:
        episodes = np.arange(len(per_episode))
        fig, ax = plt.subplots(1, 1, figsize=(12, 6))
        ax.plot(episodes, [row["kl_ours"] for row in per_episode], label="Ours", marker="o")
        ax.plot(episodes, [row["kl_lower"] for row in per_episode], label="Lower bound", marker="x")
        ax.plot(episodes, [row["kl_upper"] for row in per_episode], label="Upper bound", marker="^")
        ax.set_ylabel("KL Divergence")
        ax.set_xlabel("Episode index")
        ax.legend()
        ax.grid(True, linestyle="--", alpha=0.6)
        plt.tight_layout()
        if save_plot:
            plt.savefig(save_plot, dpi=150)
            print(f"Saved plot to {save_plot}")
        else:
            plt.show()

    return per_episode

from torch.utils.data import DataLoader
from scipy.stats import multivariate_normal

def collate_with_gmmparams(batch):
    """
    Custom collate function for EpisodicGMM dataset.
    Keeps gmm_params as a list instead of stacking (since K varies per episode).
    """
    Xc, Xq, ep_stats, gmm_params = zip(*batch)
    return (
        torch.stack(Xc, dim=0),       # (B, Nc, D)
        torch.stack(Xq, dim=0),       # (B, Nq, D)
        torch.stack(ep_stats, dim=0), # (B, 2D)
        list(gmm_params),             # list of dicts, one per episode
    )


if __name__ == "__main__":
    # === Load dataset ===
    test_dataset = EpisodicGMM(
        n_episodes=200,   # number of test episodes
        dim=2,
        Nc=16,
        Nq=64,
        seed=42
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=8,
        shuffle=False,
        collate_fn=collate_with_gmmparams
    )

    # === Load trained model ===
    model = SetTransformerMDN(
        dim=2, d_model=128, nhead=8, nlayers=3,
        K_mdn=5, add_ep_stats=True
    ).to(DEVICE)

    model.load_state_dict(torch.load("icde_fmq_best.pt", map_location=DEVICE))
    model.eval()

    # === Run KL evaluation ===
    results = evaluate_kl_bounds(
        model=model,
        dataloader=test_loader,
        device=DEVICE,
        n_batches=10,             # number of batches to evaluate
        n_mdn_samples=500,        # MC samples per episode
        seed=0,
        plot=True,
        save_plot="kl_results.png"
    )

    print(f"\nEvaluated {len(results)} episodes")