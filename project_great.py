# -*- coding: utf-8 -*-
"""Project great.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FkddkT9q2S5UD6_IfwpLGA0FtIAju8DF
"""

# train_icde_fmq.py
# ------------------------------------------------------------
# In-Context Density Estimation with Fully Masked Queries only
# Context is fully visible; queries are all masked.
# The model must learn p(x | context) and is trained via MDN NLL.
# ------------------------------------------------------------
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# -----------------------------
# Config
# -----------------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
DTYPE = torch.float32
EPS = 1e-8


# =================================
# Episodic GMM meta-dataset (with params)
# =================================
class EpisodicGMM(Dataset):
    """
    Each __getitem__ returns one episode:
      - Xc: (Nc, D) context samples (fully visible)
      - Xq: (Nq, D) query samples (used as supervision only)
      - ep_stats: (2D,) [context mean, context std] for per-episode normalization
      - gmm_params: dictionary of true GMM parameters (pis, means, sigmas)
    """
    def __init__(self, n_episodes=20000, dim=2, K_max=5, Nc=16, Nq=64,
                 normalize_episode=True, seed=0):
        self.n_episodes = n_episodes
        self.dim = dim
        self.K_max = K_max
        self.Nc = Nc
        self.Nq = Nq
        self.normalize_episode = normalize_episode
        self.rng = np.random.default_rng(seed)

    def _sample_gmm(self):
        K = self.rng.integers(2, self.K_max + 1)
        pis = self.rng.dirichlet(np.ones(K))
        means = self.rng.normal(0, 5, size=(K, self.dim))
        # diagonal covariances (per-dim std in [0.3, 2.0])
        sigmas = 0.3 + self.rng.random((K, self.dim)) * 1.7
        return K, pis, means.astype(np.float32), sigmas.astype(np.float32)

    def _sample_points(self, K, pis, means, sigmas, N):
        comp = self.rng.choice(K, size=N, p=pis)
        X = means[comp] + self.rng.standard_normal((N, self.dim)) * sigmas[comp]
        return X.astype(np.float32)

    def __len__(self):
        return self.n_episodes

    def __getitem__(self, idx):
        K, pis, means, sigs = self._sample_gmm()
        Xc = self._sample_points(K, pis, means, sigs, self.Nc)
        Xq = self._sample_points(K, pis, means, sigs, self.Nq)

        if self.normalize_episode:
            mu = Xc.mean(axis=0, keepdims=True)
            std = Xc.std(axis=0, keepdims=True) + 1e-6
            Xc = (Xc - mu) / std
            Xq = (Xq - mu) / std
            means = (means - mu) / std
            sigs = sigs / std
            ep_stats = np.concatenate([mu, std], axis=-1).astype(np.float32)  # (1, 2D)
        else:
            ep_stats = np.zeros((1, 2 * self.dim), dtype=np.float32)

        gmm_params = {
            "pis": torch.tensor(pis, dtype=torch.float32),       # (K,)
            "means": torch.tensor(means, dtype=torch.float32),   # (K, D)
            "sigmas": torch.tensor(sigs, dtype=torch.float32)    # (K, D)
        }

        return (
            torch.tensor(Xc, dtype=torch.float32),          # (Nc, D)
            torch.tensor(Xq, dtype=torch.float32),          # (Nq, D)
            torch.tensor(ep_stats.squeeze(0), dtype=torch.float32),  # (2D,)
            gmm_params,
        )

import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# Assuming you already have EpisodicGMM defined as in your code
dataset = EpisodicGMM(n_episodes=1, dim=2, K_max=5, Nc=50, Nq=200)
Xc, Xq, _, _ = dataset[0]  # one episode

# Convert to numpy
Xc = Xc.numpy()
Xq = Xq.numpy()

plt.figure(figsize=(6, 6))
plt.scatter(Xc[:, 0], Xc[:, 1], c='red', marker='o', label="Context (Xc)")
plt.scatter(Xq[:, 0], Xq[:, 1], c='blue', marker='x', alpha=0.6, label="Query (Xq)")
plt.title("Episodic GMM Dataset (Unmasked)")
plt.xlabel("x₁")
plt.ylabel("x₂")
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Take one episode
dataset = EpisodicGMM(n_episodes=1, dim=2, K_max=5, Nc=50, Nq=200)
Xc, Xq, _, _ = dataset[0]

# Convert to numpy
Xc = Xc.numpy()
Xq = Xq.numpy()

fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Left: Unmasked
axes[0].scatter(Xc[:, 0], Xc[:, 1], c='red', marker='o', label="Context (Xc)")
axes[0].scatter(Xq[:, 0], Xq[:, 1], c='blue', marker='x', alpha=0.6, label="Query (Xq)")
axes[0].set_title("Unmasked Episode (Full Data)")
axes[0].set_xlabel("x₁")
axes[0].set_ylabel("x₂")
axes[0].legend()
axes[0].grid(True)

# Right: Masked (queries hidden)
axes[1].scatter(Xc[:, 0], Xc[:, 1], c='red', marker='o', label="Context (Xc)")
# Plot queries as hollow gray circles (masked)
axes[1].scatter(Xq[:, 0], Xq[:, 1], facecolors='none', edgecolors='gray', alpha=0.6, label="Masked Query (Xq)")
axes[1].set_title("Masked Episode (What Model Sees)")
axes[1].set_xlabel("x₁")
axes[1].set_ylabel("x₂")
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

# train_icde_fmq.py
# ------------------------------------------------------------
# In-Context Density Estimation with Fully Masked Queries only
# Context is fully visible; queries are all masked.
# The model must learn p(x | context) and is trained via MDN NLL.
# ------------------------------------------------------------
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, default_collate # Import default_collate

# -----------------------------
# Config
# -----------------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
DTYPE = torch.float32
EPS = 1e-8


# =================================
# Episodic GMM meta-dataset (with params)
# =================================
class EpisodicGMM(Dataset):
    """
    Each __getitem__ returns one episode:
      - Xc: (Nc, D) context samples (fully visible)
      - Xq: (Nq, D) query samples (used as supervision only)
      - ep_stats: (2D,) [context mean, context std] for per-episode normalization
      - gmm_params: dictionary of true GMM parameters (pis, means, sigmas)
    """
    def __init__(self, n_episodes=20000, dim=2, K_max=5, Nc=16, Nq=64,
                 normalize_episode=True, seed=0):
        self.n_episodes = n_episodes
        self.dim = dim
        self.K_max = K_max
        self.Nc = Nc
        self.Nq = Nq
        self.normalize_episode = normalize_episode
        self.rng = np.random.default_rng(seed)

    def _sample_gmm(self):
        K = self.rng.integers(2, self.K_max + 1)
        pis = self.rng.dirichlet(np.ones(K))
        means = self.rng.normal(0, 5, size=(K, self.dim))
        # diagonal covariances (per-dim std in [0.3, 2.0])
        sigmas = 0.3 + self.rng.random((K, self.dim)) * 1.7
        return K, pis, means.astype(np.float32), sigmas.astype(np.float32)

    def _sample_points(self, K, pis, means, sigmas, N):
        comp = self.rng.choice(K, size=N, p=pis)
        X = means[comp] + self.rng.standard_normal((N, self.dim)) * sigmas[comp]
        return X.astype(np.float32)

    def __len__(self):
        return self.n_episodes

    def __getitem__(self, idx):
        K, pis, means, sigs = self._sample_gmm()
        Xc = self._sample_points(K, pis, means, sigs, self.Nc)
        Xq = self._sample_points(K, pis, means, sigs, self.Nq)

        if self.normalize_episode:
            mu = Xc.mean(axis=0, keepdims=True)
            std = Xc.std(axis=0, keepdims=True) + 1e-6
            Xc = (Xc - mu) / std
            Xq = (Xq - mu) / std
            means = (means - mu) / std
            sigs = sigs / std
            ep_stats = np.concatenate([mu, std], axis=-1).astype(np.float32)  # (1, 2D)
        else:
            ep_stats = np.zeros((1, 2 * self.dim), dtype=np.float32)

        gmm_params = {
            "pis": torch.tensor(pis, dtype=torch.float32),       # (K,)
            "means": torch.tensor(means, dtype=torch.float32),   # (K, D)
            "sigmas": torch.tensor(sigs, dtype=torch.float32)    # (K, D)
        }

        return (
            torch.tensor(Xc, dtype=torch.float32),          # (Nc, D)
            torch.tensor(Xq, dtype=torch.float32),          # (Nq, D)
            torch.tensor(ep_stats.squeeze(0), dtype=torch.float32),  # (2D,)
            gmm_params,
        )


# =======================================
# Permutation-invariant Transformer (MDN)
# =======================================
class SetTransformerMDN(nn.Module):
    """
    Inputs are built per token as: [x*m, m, (mu_ctx, std_ctx)].
    - Context tokens: m = 1 (fully visible), x*m = x
    - Query tokens:   m = 0 (fully masked),   x*m = 0
    We concatenate context and query tokens (no positional encoding),
    encode with a Transformer, then predict MDN params for query tokens.
    """
    def __init__(self, dim=2, d_model=192, nhead=8, nlayers=3, K_mdn=5, add_ep_stats=True):
        super().__init__()
        self.dim = dim
        self.d_model = d_model
        self.K = K_mdn
        self.add_ep_stats = add_ep_stats

        in_dim = dim + dim  # [x*m, m]
        if add_ep_stats:
            in_dim += 2 * dim  # [mu_ctx, std_ctx]

        self.input_proj = nn.Linear(in_dim, d_model)
        self.seg_embed = nn.Embedding(2, d_model)  # 0=context, 1=query

        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead,
            dim_feedforward=d_model * 4, batch_first=True,
            dropout=0.1, activation="gelu"
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers)

        self.query_proj = nn.Linear(d_model, d_model)
        self.pi_head = nn.Linear(d_model, self.K)
        self.mu_head = nn.Linear(d_model, self.K * dim)
        self.logsig_head = nn.Linear(d_model, self.K * dim)

    def _build_tokens(self, X, M, seg_id, ep_stats):
        # X, M: (B, N, D) ; ep_stats: (B, 2D) or None
        feats = [X * M, M]
        if self.add_ep_stats and ep_stats is not None:
            ep = ep_stats.unsqueeze(1).expand(X.size(0), X.size(1), ep_stats.size(-1))
            feats.append(ep)
        Z = torch.cat(feats, dim=-1)  # (B, N, in_dim)
        H = self.input_proj(Z) + self.seg_embed.weight[seg_id]
        return H

    def forward(self, Xc, Xq, ep_stats):
        """
        Xc: (B, Nc, D) context (fully visible)
        Xq: (B, Nq, D) query values (only used to create query tokens; they are fully masked)
        ep_stats: (B, 2D)
        """
        B, Nc, D = Xc.shape
        Nq = Xq.shape[1]

        # Context masks = 1, Query masks = 0 (fully masked queries)
        Mc = torch.ones(B, Nc, D, device=Xc.device, dtype=Xc.dtype)
        Mq = torch.zeros(B, Nq, D, device=Xq.device, dtype=Xq.dtype)

        tok_c = self._build_tokens(Xc, Mc, seg_id=0, ep_stats=ep_stats)   # (B, Nc, d)
        tok_q = self._build_tokens(torch.zeros_like(Xq), Mq, seg_id=1, ep_stats=ep_stats)  # zeros, m=0

        # Concatenate (keep order: [context | queries])
        tokens = torch.cat([tok_c, tok_q], dim=1)  # (B, Nc+Nq, d)

        # Encode
        H = self.encoder(tokens)
        Hq = H[:, Nc:, :]                           # (B, Nq, d) just the query slice
        Hq = F.gelu(self.query_proj(Hq))

        pi_logits = self.pi_head(Hq)                         # (B, Nq, K)
        mu = self.mu_head(Hq).view(B, Nq, self.K, D)         # (B, Nq, K, D)
        logsig = self.logsig_head(Hq).view(B, Nq, self.K, D) # (B, Nq, K, D)
        sig = torch.exp(logsig).clamp_min(1e-4)
        return pi_logits, mu, sig


# ===============================
# MDN Negative Log-Likelihood
# ===============================
def mdn_nll_gaussian(pi_logits, mu, sig, y):
    """
    pi_logits: (B, Nq, K)
    mu, sig:   (B, Nq, K, D)
    y:         (B, Nq, D)
    """
    log_pi = torch.log_softmax(pi_logits, dim=-1)                # (B, Nq, K)
    y = y.unsqueeze(2)                                           # (B, Nq, 1, D)
    var = sig ** 2
    diff = (y - mu)
    comp_logprob = -0.5 * (
        (diff**2 / var).sum(-1) + torch.log(2 * math.pi * var + EPS).sum(-1)
    )                                                            # (B, Nq, K)
    log_mix = torch.logsumexp(log_pi + comp_logprob, dim=-1)     # (B, Nq)
    return -(log_mix).mean()


# === Custom collate function for training ===
# This collate function will ignore the gmm_params dictionary
def train_collate_fn(batch):
    # The batch will contain tuples of (Xc, Xq, ep_stats, gmm_params)
    # We only want to collate the first three elements for training
    return default_collate([item[:3] for item in batch])


# =========================
# Training (Fully Masked Q)
# =========================
def train_icde_fmq(
    epochs=40,
    batch_size=32,
    Nc=16, Nq=64, dim=2,
    lr=1e-3, weight_decay=1e-4,
    K_mdn=5,
    entropy_bonus=1e-3,
    seed=0,
):
    # Data
    train_set = EpisodicGMM(n_episodes=20000, dim=dim, Nc=Nc, Nq=Nq, seed=seed)
    val_set   = EpisodicGMM(n_episodes=1000,  dim=dim, Nc=Nc, Nq=Nq, seed=seed + 1)

    train_loader = DataLoader(
        train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True,
        collate_fn=train_collate_fn # Use the custom collate function
    )
    val_loader = DataLoader(
        val_set, batch_size=batch_size, shuffle=False, pin_memory=True,
        collate_fn=train_collate_fn # Use the custom collate function for validation too
    )

    # Model / Optim
    model = SetTransformerMDN(
        dim=dim, d_model=128, nhead=8, nlayers=3, K_mdn=K_mdn, add_ep_stats=True
    ).to(DEVICE)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    best_val = float("inf")

    for ep in range(1, epochs + 1):
        # ------------- Train -------------
        model.train()
        tr_loss = 0.0
        # The train_collate_fn returns only Xc, Xq, ep_stats
        for Xc, Xq, ep_stats in train_loader:
            Xc, Xq, ep_stats = Xc.to(DEVICE), Xq.to(DEVICE), ep_stats.to(DEVICE)

            # Forward: queries are fully masked inside the model
            pi_logits, mu, sig = model(Xc, Xq, ep_stats)

            # NLL on true query targets (teacher forcing)
            nll = mdn_nll_gaussian(pi_logits, mu, sig, Xq)

            # small entropy bonus on sigmas to avoid variance collapse
            ent = 0.5 * torch.log(sig**2 + EPS).mean()
            loss = nll - entropy_bonus * ent

            opt.zero_grad()
            loss.backward()
            opt.step()

            tr_loss += loss.item() * Xc.size(0)

        tr_loss /= len(train_loader.dataset)

        # ------------- Validation -------------
        model.eval()
        with torch.no_grad():
            val_loss = 0.0
            # The train_collate_fn returns only Xc, Xq, ep_stats
            for Xc, Xq, ep_stats in val_loader:
                Xc, Xq, ep_stats = Xc.to(DEVICE), Xq.to(DEVICE), ep_stats.to(DEVICE)
                pi_logits, mu, sig = model(Xc, Xq, ep_stats)
                nll = mdn_nll_gaussian(pi_logits, mu, sig, Xq)
                ent = 0.5 * torch.log(sig**2 + EPS).mean()
                loss = nll - entropy_bonus * ent
                val_loss += loss.item() * Xc.size(0)
            val_loss /= len(val_loader.dataset)

        print(f"Epoch {ep:03d} | Train Obj: {tr_loss:.4f} | Val Obj: {val_loss:.4f}")

        if val_loss < best_val - 1e-6:
            best_val = val_loss
            torch.save(model.state_dict(), "icde_fmq_best.pt")

    print(f"✅ Best Val Objective: {best_val:.4f}")
    return model


# -----------------------------
# Run
# -----------------------------
if __name__ == "__main__":
    _ = train_icde_fmq(
        epochs=40,          # you can start with 30–40; bump to 60 if still underfitting
        batch_size=32,
        Nc=16, Nq=64, dim=2,
        lr=1e-3, weight_decay=1e-4,
        K_mdn=5,
        entropy_bonus=1e-3,
        seed=0,
    )

import torch
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader, default_collate
from scipy.stats import multivariate_normal, gaussian_kde
import math

# === Utility: compute analytic GMM density ===
def compute_gmm_density(grid_points, weights, means, covs):
    """Compute GMM density at grid points (2D)."""
    density = np.zeros(len(grid_points))
    for w, mu, cov in zip(weights, means, covs):
        rv = multivariate_normal(mean=mu, cov=cov)
        density += w * rv.pdf(grid_points)
    return density

# === Utility: compute contour levels for probability mass ===
def get_density_levels(Z, levels=[0.1, 0.3, 0.5, 0.9]):
    """Return contour thresholds for specified probability mass levels."""
    Z_flat = Z.ravel()
    Z_sorted = np.sort(Z_flat)[::-1]
    cumsum = np.cumsum(Z_sorted)
    cumsum /= cumsum[-1]

    thresholds = []
    for p in levels:
        idx = np.searchsorted(cumsum, p)
        thresholds.append(Z_sorted[idx])
    return thresholds

# === Custom collate function to handle gmm_params ===
def custom_collate_fn(batch):
    Xc_batch = [item[0] for item in batch]
    Xq_batch = [item[1] for item in batch]
    ep_stats_batch = [item[2] for item in batch]
    gmm_params_batch = [item[3] for item in batch]

    collated_data = default_collate([
        (Xc, Xq, ep_stats) for Xc, Xq, ep_stats, _ in batch
    ])

    return collated_data[0], collated_data[1], collated_data[2], gmm_params_batch


@torch.no_grad()
def evaluate_icde_fmq(
    model_path,
    dataloader,
    device="cuda",
    n_batches=5,
    save_prefix="eval",
    save_format="png"
):
    # === Load trained model ===
    model = SetTransformerMDN(dim=2, d_model=128, nhead=8, nlayers=3,
                              K_mdn=5, add_ep_stats=True).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    all_scatter_data, all_contour_data, all_hist_data = [], [], []

    for batch_idx, (Xc, Xq, ep_stats, gmm_params_batch) in enumerate(dataloader):
        if batch_idx >= n_batches:
            break
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        B, Nc, D = Xc.shape
        Nq = Xq.shape[1]
        K = model.K

        # Context fully visible, queries masked
        Mc = torch.ones(B, Nc, D, device=device, dtype=Xc.dtype)
        Mq = torch.zeros(B, Nq, D, device=device, dtype=Xq.dtype)
        Xq_masked = Xq * Mq

        # Forward pass
        pi_logits, mu, sig = model(Xc, Xq_masked, ep_stats)

        # De-normalize
        mu_ctx = ep_stats[:, :D].unsqueeze(1)
        std_ctx = ep_stats[:, D:].unsqueeze(1)
        Xq_denorm = Xq * std_ctx + mu_ctx

        for b in range(min(2, B)):  # show up to 2 episodes per batch
            true_xy = Xq_denorm[b].cpu().numpy()

            # Ground truth GMM (from dataset)
            gmm_params = gmm_params_batch[b]
            pis_true = gmm_params["pis"].cpu().numpy()
            means_true = gmm_params["means"].cpu().numpy()
            sigmas_true = gmm_params["sigmas"].cpu().numpy()
            means_true = means_true * std_ctx[b, 0].cpu().numpy() + mu_ctx[b, 0].cpu().numpy()
            sigmas_true = sigmas_true * std_ctx[b, 0].cpu().numpy()
            covs_true = [np.diag(s**2) for s in sigmas_true]

                        # Predicted MDN parameters
            pi_pred = torch.softmax(pi_logits[b], dim=-1).mean(dim=0).cpu().numpy()
            mu_pred = mu[b].mean(dim=0).cpu().numpy()
            sig_pred = sig[b].mean(dim=0).cpu().numpy()
            covs_pred = [np.diag(s**2) for s in sig_pred]

            # Save for plotting
            all_scatter_data.append((true_xy, mu_pred, f"Episode {batch_idx}, Batch {b} (Scatter)"))
            all_contour_data.append((true_xy, pis_true, means_true, covs_true,
                                     pi_pred, mu_pred, covs_pred,
                                     f"Episode {batch_idx}, Batch {b} (Contours)"))
            all_hist_data.append((true_xy, pi_pred, mu_pred, sig_pred,
                                  mu_ctx[b, 0].cpu().numpy(), std_ctx[b, 0].cpu().numpy(),
                                  f"Episode {batch_idx}, Batch {b} (Histograms)"))

    # === Scatter plots ===
    if all_scatter_data:
        ncols = min(5, len(all_scatter_data))
        nrows = math.ceil(len(all_scatter_data) / ncols)
        fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 5, nrows * 5))
        axes = axes.flatten()
        for i, (true_xy, mu_pred, title) in enumerate(all_scatter_data):
            ax = axes[i]
            ax.scatter(true_xy[:, 0], true_xy[:, 1], alpha=0.6, label="True")
            ax.scatter(mu_pred[:, 0], mu_pred[:, 1], alpha=0.6, label="Pred Means")
            ax.legend(); ax.set_title(title)
        for j in range(i+1, len(axes)):
            fig.delaxes(axes[j])
        plt.tight_layout()
        fig.savefig(f"{save_prefix}_scatter.{save_format}", bbox_inches="tight")
        plt.close(fig)

    # === Contour plots ===
    if all_contour_data:
        ncols = min(5, len(all_contour_data))
        nrows = math.ceil(len(all_contour_data) / ncols)
        fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 5, nrows * 5))
        axes = axes.flatten()
        for i, (true_xy, pis_true, means_true, covs_true,
                pi_pred, mu_pred, covs_pred, title) in enumerate(all_contour_data):
            ax = axes[i]

            # Grid
            mins, maxs = true_xy.min(axis=0)-1.0, true_xy.max(axis=0)+1.0
            xs, ys = np.linspace(mins[0], maxs[0], 200), np.linspace(mins[1], maxs[1], 200)
            Xg, Yg = np.meshgrid(xs, ys)
            grid = np.column_stack([Xg.ravel(), Yg.ravel()])

            # Densities
            Zt = compute_gmm_density(grid, pis_true, means_true, covs_true).reshape(Xg.shape)
            Zp = compute_gmm_density(grid, pi_pred, mu_pred, covs_pred).reshape(Xg.shape)

            # Levels (probability mass contours)
            levels_t = get_density_levels(Zt)
            levels_p = get_density_levels(Zp)

            # Plot samples
            ax.scatter(true_xy[:, 0], true_xy[:, 1], s=10, c="black", alpha=0.4, label="Samples")

            cs1 = ax.contour(Xg, Yg, Zt, levels=levels_t, linewidths=1.4, cmap="Blues")
            cs2 = ax.contour(Xg, Yg, Zp, levels=levels_p, linestyles="--", linewidths=1.4, cmap="Reds")
            ax.set_title(title); ax.set_xlabel("X"); ax.set_ylabel("Y"); ax.legend()
        for j in range(i+1, len(axes)):
            fig.delaxes(axes[j])
        plt.tight_layout()
        fig.savefig(f"{save_prefix}_contours.{save_format}", bbox_inches="tight")
        plt.close(fig)

    # === Histograms with KDE ===
    if all_hist_data:
        ncols = min(5, len(all_hist_data))
        nrows = math.ceil(len(all_hist_data) / ncols)
        D = 2
        fig, axes = plt.subplots(nrows*D, ncols, figsize=(ncols * 5, nrows * D * 3))
        axes = axes.flatten()
        for i, (true_xy, pi_pred, mu_pred, sig_pred, mu_ctx, std_ctx, title) in enumerate(all_hist_data):
            # Sample from predicted MDN
            comp = torch.distributions.Independent(torch.distributions.Normal(
                torch.tensor(mu_pred), torch.tensor(sig_pred)), 1)
            mix = torch.distributions.Categorical(probs=torch.tensor(pi_pred))
            mdn = torch.distributions.MixtureSameFamily(mix, comp)
            pred_xy = mdn.sample((true_xy.shape[0],)).numpy()
            pred_xy = pred_xy * std_ctx + mu_ctx

            for d in range(D):
                ax = axes[i*D+d]
                ax.hist(true_xy[:, d], bins=30, alpha=0.5, density=True, label="True")
                ax.hist(pred_xy[:, d], bins=30, alpha=0.5, density=True, label="Pred")

                kde_true = gaussian_kde(true_xy[:, d])
                kde_pred = gaussian_kde(pred_xy[:, d])
                xs = np.linspace(min(true_xy[:, d].min(), pred_xy[:, d].min())-1,
                                 max(true_xy[:, d].max(), pred_xy[:, d].max())+1, 200)
                ax.plot(xs, kde_true(xs), "b-", label="True KDE")
                ax.plot(xs, kde_pred(xs), "r--", label="Pred KDE")

                ax.set_title(f"{title} - Dim {['X','Y'][d]}")
                ax.legend()
        for j in range(i*D+d+1, len(axes)):
            fig.delaxes(axes[j])
        plt.tight_layout()
        fig.savefig(f"{save_prefix}_histograms.{save_format}", bbox_inches="tight")
        plt.close(fig)

    print("✅ Evaluation complete: scatter, contour, and histogram plots saved.")

# === Example call ===
if __name__ == "__main__":
    # Build test dataset
    test_dataset = EpisodicGMM(
        n_episodes=100,   # number of episodes
        dim=2,            # 2D GMMs
        Nc=16,            # context size
        Nq=64,            # query size
        seed=42
    )

    # Dataloader with custom collate (to keep gmm_params)
    test_loader = DataLoader(
        test_dataset,
        batch_size=16,
        shuffle=True,
        collate_fn=custom_collate_fn
    )

    # Run evaluation
    evaluate_icde_fmq(
        model_path="icde_fmq_best.pt",  # trained model checkpoint
        dataloader=test_loader,
        device="cuda",                  # or "cpu"
        n_batches=5,                    # number of batches to visualize
        save_prefix="eval",             # prefix for saved plots
        save_format="png"               # or "pdf", "svg"
    )

import torch
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader, default_collate
from scipy.stats import multivariate_normal, gaussian_kde
import math

# === Utility: compute analytic GMM density ===
def compute_gmm_density(grid_points, weights, means, covs):
    """Compute GMM density at grid points (2D)."""
    density = np.zeros(len(grid_points))
    for w, mu, cov in zip(weights, means, covs):
        rv = multivariate_normal(mean=mu, cov=cov)
        density += w * rv.pdf(grid_points)
    return density

# === Utility: compute contour levels for probability mass ===
def get_density_levels(Z, levels=[0.1, 0.3, 0.5, 0.9]):
    """Return contour thresholds for specified probability mass levels."""
    Z_flat = Z.ravel()
    Z_sorted = np.sort(Z_flat)[::-1]
    cumsum = np.cumsum(Z_sorted)
    cumsum /= (cumsum[-1] + 1e-8) # Add small epsilon to avoid division by zero

    thresholds = []
    for p in levels:
        idx = np.searchsorted(cumsum, p)
        if idx < len(Z_sorted):
            thresholds.append(Z_sorted[idx])
        else:
            thresholds.append(Z_sorted[-1]) # Use the minimum density if probability mass exceeds max

    # Ensure levels are unique and increasing
    thresholds = sorted(list(set(thresholds)))
    return thresholds

# === Custom collate function to handle gmm_params ===
def custom_collate_fn(batch):
    Xc_batch = [item[0] for item in batch]
    Xq_batch = [item[1] for item in batch]
    ep_stats_batch = [item[2] for item in batch]
    gmm_params_batch = [item[3] for item in batch]

    collated_data = default_collate([
        (Xc, Xq, ep_stats) for Xc, Xq, ep_stats, _ in batch
    ])

    return collated_data[0], collated_data[1], collated_data[2], gmm_params_batch


@torch.no_grad()
def evaluate_icde_fmq(
    model_path,
    dataloader,
    device="cuda",
    n_batches=5,
    save_prefix="eval",
    save_format="png"
):
    # === Load trained model ===
    model = SetTransformerMDN(dim=2, d_model=128, nhead=8, nlayers=3,
                              K_mdn=5, add_ep_stats=True).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    all_scatter_data, all_contour_data, all_hist_data = [], [], []

    for batch_idx, (Xc, Xq, ep_stats, gmm_params_batch) in enumerate(dataloader):
        if batch_idx >= n_batches:
            break
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        B, Nc, D = Xc.shape
        Nq = Xq.shape[1]
        K = model.K

        # Context fully visible, queries masked
        Mc = torch.ones(B, Nc, D, device=device, dtype=Xc.dtype)
        Mq = torch.zeros(B, Nq, D, device=device, dtype=Xq.dtype)
        Xq_masked = Xq * Mq

        # Forward pass
        pi_logits, mu, sig = model(Xc, Xq_masked, ep_stats)

        # De-normalize
        mu_ctx = ep_stats[:, :D].unsqueeze(1)
        std_ctx = ep_stats[:, D:].unsqueeze(1)
        Xq_denorm = Xq * std_ctx + mu_ctx

        for b in range(min(2, B)):  # show up to 2 episodes per batch
            true_xy = Xq_denorm[b].cpu().numpy()

            # Ground truth GMM (from dataset)
            gmm_params = gmm_params_batch[b]
            pis_true = gmm_params["pis"].cpu().numpy()
            means_true = gmm_params["means"].cpu().numpy()
            sigmas_true = gmm_params["sigmas"].cpu().numpy()
            means_true = means_true * std_ctx[b, 0].cpu().numpy() + mu_ctx[b, 0].cpu().numpy()
            sigmas_true = sigmas_true * std_ctx[b, 0].cpu().numpy()
            covs_true = [np.diag(s**2) for s in sigmas_true]

                        # Predicted MDN parameters
            pi_pred = torch.softmax(pi_logits[b], dim=-1).mean(dim=0).cpu().numpy()
            mu_pred = mu[b].mean(dim=0).cpu().numpy()
            sig_pred = sig[b].mean(dim=0).cpu().numpy()
            covs_pred = [np.diag(s**2) for s in sig_pred]

            # Save for plotting
            all_scatter_data.append((true_xy, mu_pred, f"Episode {batch_idx}, Batch {b} (Scatter)"))
            all_contour_data.append((true_xy, pis_true, means_true, covs_true,
                                     pi_pred, mu_pred, covs_pred,
                                     f"Episode {batch_idx}, Batch {b} (Contours)"))
            all_hist_data.append((true_xy, pi_pred, mu_pred, sig_pred,
                                  mu_ctx[b, 0].cpu().numpy(), std_ctx[b, 0].cpu().numpy(),
                                  f"Episode {batch_idx}, Batch {b} (Histograms)"))

    # === Scatter plots ===
    if all_scatter_data:
        ncols = min(5, len(all_scatter_data))
        nrows = math.ceil(len(all_scatter_data) / ncols)
        fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 5, nrows * 5))
        axes = axes.flatten()
        for i, (true_xy, mu_pred, title) in enumerate(all_scatter_data):
            ax = axes[i]
            ax.scatter(true_xy[:, 0], true_xy[:, 1], alpha=0.6, label="True")
            ax.scatter(mu_pred[:, 0], mu_pred[:, 1], alpha=0.6, label="Pred Means")
            ax.legend(); ax.set_title(title)
        for j in range(i+1, len(axes)):
            fig.delaxes(axes[j])
        plt.tight_layout()
        fig.savefig(f"{save_prefix}_scatter.{save_format}", bbox_inches="tight")
        plt.close(fig)

    # === Contour plots ===
    if all_contour_data:
        ncols = min(5, len(all_contour_data))
        nrows = math.ceil(len(all_contour_data) / ncols)
        fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 5, nrows * 5))
        axes = axes.flatten()
        for i, (true_xy, pis_true, means_true, covs_true,
                pi_pred, mu_pred, covs_pred, title) in enumerate(all_contour_data):
            ax = axes[i]

            # Grid
            mins, maxs = true_xy.min(axis=0)-1.0, true_xy.max(axis=0)+1.0
            xs, ys = np.linspace(mins[0], maxs[0], 200), np.linspace(mins[1], maxs[1], 200)
            Xg, Yg = np.meshgrid(xs, ys)
            grid = np.column_stack([Xg.ravel(), Yg.ravel()])

            # Densities
            Zt = compute_gmm_density(grid, pis_true, means_true, covs_true).reshape(Xg.shape)
            Zp = compute_gmm_density(grid, pi_pred, mu_pred, covs_pred).reshape(Xg.shape)

            # Levels (probability mass contours)
            levels_t = get_density_levels(Zt)
            levels_p = get_density_levels(Zp)

            # Plot samples
            ax.scatter(true_xy[:, 0], true_xy[:, 1], s=10, c="black", alpha=0.4, label="Samples")

            cs1 = ax.contour(Xg, Yg, Zt, levels=levels_t, linewidths=1.4, cmap="Blues")
            cs2 = ax.contour(Xg, Yg, Zp, levels=levels_p, linestyles="--", linewidths=1.4, cmap="Reds")
            ax.set_title(title); ax.set_xlabel("X"); ax.set_ylabel("Y"); ax.legend()
        for j in range(i+1, len(axes)):
            fig.delaxes(axes[j])
        plt.tight_layout()
        fig.savefig(f"{save_prefix}_contours.{save_format}", bbox_inches="tight")
        plt.close(fig)

    # === Histograms with KDE ===
    if all_hist_data:
        ncols = min(5, len(all_hist_data))
        nrows = math.ceil(len(all_hist_data) / ncols)
        D = 2
        fig, axes = plt.subplots(nrows*D, ncols, figsize=(ncols * 5, nrows * D * 3))
        axes = axes.flatten()
        for i, (true_xy, pi_pred, mu_pred, sig_pred, mu_ctx, std_ctx, title) in enumerate(all_hist_data):
            # Sample from predicted MDN
            comp = torch.distributions.Independent(torch.distributions.Normal(
                torch.tensor(mu_pred), torch.tensor(sig_pred)), 1)
            mix = torch.distributions.Categorical(probs=torch.tensor(pi_pred))
            mdn = torch.distributions.MixtureSameFamily(mix, comp)
            pred_xy = mdn.sample((true_xy.shape[0],)).numpy()
            pred_xy = pred_xy * std_ctx + mu_ctx

            for d in range(D):
                ax = axes[i*D+d]
                ax.hist(true_xy[:, d], bins=30, alpha=0.5, density=True, label="True")
                ax.hist(pred_xy[:, d], bins=30, alpha=0.5, density=True, label="Pred")

                kde_true = gaussian_kde(true_xy[:, d])
                kde_pred = gaussian_kde(pred_xy[:, d])
                xs = np.linspace(min(true_xy[:, d].min(), pred_xy[:, d].min())-1,
                                 max(true_xy[:, d].max(), pred_xy[:, d].max())+1, 200)
                ax.plot(xs, kde_true(xs), "b-", label="True KDE")
                ax.plot(xs, kde_pred(xs), "r--", label="Pred KDE")

                ax.set_title(f"{title} - Dim {['X','Y'][d]}")
                ax.legend()
        for j in range(i*D+d+1, len(axes)):
            fig.delaxes(axes[j])
        plt.tight_layout()
        fig.savefig(f"{save_prefix}_histograms.{save_format}", bbox_inches="tight")
        plt.close(fig)

    print("✅ Evaluation complete: scatter, contour, and histogram plots saved.")

# === Calling function ===
if __name__ == "__main__":
    from torch.utils.data import DataLoader
    # Assume you already have dataset object e.g. `test_dataset`
    test_dataset = EpisodicGMM(n_episodes=100, dim=2, Nc=16, Nq=64, seed=42) # Define test_dataset
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)

    evaluate_icde_fmq(
        model_path="icde_fmq_best.pt",
        dataloader=test_loader,
        device="cuda",   # or "cpu"
        n_batches=5
    )

import torch
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import wasserstein_distance_nd
from typing import Optional, List, Dict
import csv
from sklearn.mixture import GaussianMixture
from dataclasses import dataclass

# === MMD (with Gaussian kernel) ===
def compute_mmd(x, y, sigma=1.0):
    xx = np.exp(-np.square(np.linalg.norm(x[:, None, :] - x[None, :, :], axis=-1)) / (2 * sigma**2)).mean()
    yy = np.exp(-np.square(np.linalg.norm(y[:, None, :] - y[None, :, :], axis=-1)) / (2 * sigma**2)).mean()
    xy = np.exp(-np.square(np.linalg.norm(x[:, None, :] - y[None, :, :], axis=-1)) / (2 * sigma**2)).mean()
    return xx + yy - 2 * xy

# === Per-episode GMM fitting (BIC) ===
def fit_ep_gmm(X, k_min=1, k_max=6, seed=0):
    best_bic = np.inf
    best_gmm = None
    for k in range(k_min, k_max + 1):
        gmm = GaussianMixture(n_components=k, random_state=seed, reg_covar=1e-6)
        gmm.fit(X)
        bic = gmm.bic(X)
        if bic < best_bic:
            best_bic = bic
            best_gmm = gmm
    return best_gmm

# === Fit global Gaussian ===
def fit_global_gaussian(list_of_arrays):
    all_data = np.concatenate(list_of_arrays, axis=0)
    mu = np.mean(all_data, axis=0)
    cov = np.cov(all_data, rowvar=False)
    return mu, cov

def sample_gaussian(mu, cov, n, rng):
    return rng.multivariate_normal(mu, cov, size=n)

# === Metrics container (no KL now) ===
@dataclass
class JointMetrics:
    wd: float
    mmd: float

# === Evaluate model with bounds ===
@torch.no_grad()
def evaluate_with_bounds(
    model,
    dataloader,
    device="cuda",
    n_batches=5,
    seed: int = 0,
    save_csv: Optional[str] = None,
    upper_kmax: int = 6,
    plot: bool = True,
    save_plot: Optional[str] = None
):
    rng = np.random.default_rng(seed)

    # ---- Pass 1: fit global Gaussian ----
    all_true, cache_batches, taken = [], [], 0
    for batch_idx, (Xc, Xq, ep_stats) in enumerate(dataloader):
        if taken >= n_batches: break
        cache_batches.append((Xc, Xq, ep_stats))
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        D = Xc.shape[-1]
        mu_ctx  = ep_stats[:, :D].unsqueeze(1)
        std_ctx = ep_stats[:, D:].unsqueeze(1)
        Xq_denorm = (Xq * std_ctx + mu_ctx).cpu().numpy()
        for b in range(Xc.shape[0]):
            all_true.append(Xq_denorm[b])
        taken += 1

    mu_glob, cov_glob = fit_global_gaussian(all_true)

    # ---- Pass 2: per-episode metrics ----
    per_episode: List[Dict[str, float]] = []
    taken = 0
    for (Xc, Xq, ep_stats) in cache_batches:
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        B, Nc, D = Xc.shape
        Nq = Xq.shape[1]
        pi_logits, mu, sig = model(Xc, Xq, ep_stats)
        K = mu.shape[2]

        mu_rs = mu.view(B * Nq, K, D)
        sig_rs = sig.view(B * Nq, K, D)
        comp = torch.distributions.Independent(torch.distributions.Normal(mu_rs, sig_rs), 1)
        mix = torch.distributions.Categorical(logits=pi_logits.view(B * Nq, K))
        gmm = torch.distributions.MixtureSameFamily(mix, comp)
        samples = gmm.sample().view(B, Nq, D)

        mu_ctx  = ep_stats[:, :D].unsqueeze(1)
        std_ctx = ep_stats[:, D:].unsqueeze(1)
        Xq_denorm = (Xq * std_ctx + mu_ctx).cpu().numpy()
        Ypred_denorm = (samples * std_ctx + mu_ctx).cpu().numpy()

        for b in range(B):
            gt = Xq_denorm[b]
            yp = Ypred_denorm[b]

            # lower bound
            low_samples = sample_gaussian(mu_glob, cov_glob, n=len(gt), rng=rng)

            # upper bound: per-episode GMM
            gmm_ep = fit_ep_gmm(gt, k_min=1, k_max=upper_kmax, seed=seed)
            up_samples = gmm_ep.sample(n_samples=len(gt))[0]

            # collect metrics
            wd_yours = wasserstein_distance_nd(gt, yp)
            mmd_yours = compute_mmd(gt, yp)

            wd_low = wasserstein_distance_nd(gt, low_samples)
            mmd_low = compute_mmd(gt, low_samples)

            wd_up = wasserstein_distance_nd(gt, up_samples)
            mmd_up = compute_mmd(gt, up_samples)

            per_episode.append({
                "wd_yours": wd_yours, "mmd_yours": mmd_yours,
                "wd_lower": wd_low,   "mmd_lower": mmd_low,
                "wd_upper": wd_up,    "mmd_upper": mmd_up,
            })
        taken += 1
        if taken >= n_batches: break

    # ---- Aggregate results ----
    def agg(key):
        vals = np.array([row[key] for row in per_episode], dtype=float)
        return float(vals.mean()), float(vals.std())

    keys = ["wd_yours","mmd_yours",
            "wd_lower","mmd_lower",
            "wd_upper","mmd_upper"]

    print("=== Per-episode metrics (first 5 rows) ===")
    for row in per_episode[:5]:
        print({k: round(row[k], 4) for k in keys})

    print("\n=== Aggregate (mean ± std) ===")
    for label in ["yours","lower","upper"]:
        wd_m, wd_s = agg(f"wd_{label}")
        mmd_m, mmd_s = agg(f"mmd_{label}")
        print(f"[{label:6}] WD: {wd_m:.4f} ± {wd_s:.4f} | MMD: {mmd_m:.4f} ± {mmd_s:.4f}")

    if save_csv is not None:
        with open(save_csv, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            for row in per_episode:
                writer.writerow(row)
        print(f"\nSaved per-episode metrics to {save_csv}")

    if plot:
        episodes = np.arange(len(per_episode))
        fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)
        metrics = ["wd", "mmd"]
        for ax, m in zip(axes, metrics):
            ax.plot(episodes, [row[f"{m}_yours"] for row in per_episode], label="Ours", marker="o")
            ax.plot(episodes, [row[f"{m}_lower"] for row in per_episode], label="Lower bound", marker="x")
            ax.plot(episodes, [row[f"{m}_upper"] for row in per_episode], label="Upper bound", marker="^")
            ax.set_ylabel(m.upper())
            ax.legend()
            ax.grid(True, linestyle="--", alpha=0.6)
        axes[-1].set_xlabel("Episode index")
        plt.tight_layout()
        if save_plot:
            plt.savefig(save_plot, dpi=150)
            print(f"Saved plot to {save_plot}")
        else:
            plt.show()

    return per_episode

from torch.utils.data import DataLoader, default_collate
import torch
import numpy as np
from typing import Optional, List, Dict
import csv
from sklearn.mixture import GaussianMixture
from scipy.stats import wasserstein_distance_nd
from dataclasses import dataclass

# === Custom collate function to handle gmm_params ===
def custom_collate_fn(batch):
    # Separate the gmm_params from the rest of the data
    Xc_list = [item[0] for item in batch]
    Xq_list = [item[1] for item in batch]
    ep_stats_list = [item[2] for item in batch]
    gmm_params_list = [item[3] for item in batch]

    # Use default_collate for the tensors
    Xc_batch = default_collate(Xc_list)
    Xq_batch = default_collate(Xq_list)
    ep_stats_batch = default_collate(ep_stats_list)


    # Return the collated tensors and the list of gmm_params dictionaries
    return Xc_batch, Xq_batch, ep_stats_batch, gmm_params_list


# build your test loader (as you already do)
test_dataset = EpisodicGMM(n_episodes=100, dim=2, Nc=16, Nq=64, seed=42)
# Apply the custom_collate_fn to handle the gmm_params
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)

# load your trained model
model = SetTransformerMDN(dim=2, d_model=128, nhead=8, nlayers=3, K_mdn=5, add_ep_stats=True).to(DEVICE)
model.load_state_dict(torch.load("icde_fmq_best.pt", map_location=DEVICE))
model.eval()

# run evaluation with bounds
_ = evaluate_with_bounds(
    model=model,
    dataloader=test_loader,
    device=DEVICE,
    n_batches=5,          # same scope you used before
    seed=0,
    save_csv="per_episode_metrics.csv",
    save_plot="per_episode_metrics.png",
    upper_kmax=6          # search K=1..6 for the episode-wise GMM
)

import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Optional, List, Dict
from sklearn.mixture import GaussianMixture
from scipy.special import logsumexp


# === Per-episode GMM fit (for "upper" baseline) ===
def fit_ep_gmm(X, k_min=1, k_max=6, seed=0):
    best_bic, best_gmm = np.inf, None
    for k in range(k_min, k_max + 1):
        gmm = GaussianMixture(
            n_components=k,
            covariance_type="full",
            reg_covar=1e-6,
            random_state=seed
        )
        gmm.fit(X)
        bic = gmm.bic(X)
        if bic < best_bic:
            best_bic, best_gmm = bic, gmm
    return best_gmm


# === Global Gaussian baseline (for "lower" baseline) ===
def fit_global_gaussian(list_of_arrays):
    all_data = np.concatenate(list_of_arrays, axis=0)
    mu = np.mean(all_data, axis=0)
    cov = np.cov(all_data, rowvar=False)
    return mu, cov


# === MDN log probability ===
def mdn_log_prob_numpy(pi_logits_np, mu_np, sig_np, x_np, eps=1e-12):
    """
    Evaluate log probability of x under an MDN (NumPy version).
    Args:
        pi_logits_np: (Nq, K)
        mu_np: (Nq, K, D)
        sig_np: (Nq, K, D)
        x_np: (N, D)
    Returns:
        log_probs: (N,) average log-likelihood across query positions
    """
    N, D = x_np.shape
    Nq, K, _ = mu_np.shape

    # Normalized mixture weights
    log_pi = pi_logits_np - logsumexp(pi_logits_np, axis=-1, keepdims=True)  # (Nq, K)

    log_probs_per_query = []
    for i in range(Nq):
        mu_i, sig_i, log_pi_i = mu_np[i], sig_np[i], log_pi[i]  # (K,D), (K,D), (K,)

        x_exp = x_np[:, None, :]        # (N,1,D)
        mu_exp = mu_i[None, :, :]       # (1,K,D)
        sig_exp = sig_i[None, :, :]     # (1,K,D)

        var = sig_exp**2
        diff = x_exp - mu_exp
        comp_term = -0.5 * (
            np.sum(diff**2 / (var + eps), axis=-1) +
            np.sum(np.log(2 * np.pi * var + eps), axis=-1)
        )  # (N,K)

        log_mix = logsumexp(log_pi_i[None, :] + comp_term, axis=-1)  # (N,)
        log_probs_per_query.append(log_mix)

    log_probs_all = np.stack(log_probs_per_query, axis=1)  # (N, Nq)
    return log_probs_all.mean(axis=1)  # average over queries


# === Ground-truth GMM log probability (diagonal covariances) ===
def true_gmm_log_prob(pis, means, sigmas, x, eps=1e-12):
    """
    Compute log p(x) for ground-truth episodic GMM with diagonal covariances.
    Args:
        pis:    (K,)
        means:  (K,D)
        sigmas: (K,D)
        x:      (N,D)
    """
    pis = np.asarray(pis)
    means = np.asarray(means)
    sigmas = np.asarray(sigmas)

    N, D = x.shape
    K = len(pis)

    log_pis = np.log(pis + eps)
    var = sigmas**2

    # (N,K,D)
    diff = x[:, None, :] - means[None, :, :]
    quad = np.sum(diff**2 / (var[None, :, :] + eps), axis=-1)  # (N,K)
    log_det = np.sum(np.log(2 * np.pi * var + eps), axis=-1)   # (K,)

    comp = -0.5 * (quad + log_det[None, :])  # (N,K)
    return logsumexp(log_pis[None, :] + comp, axis=-1)  # (N,)


# === KL via Monte Carlo (Hershey & Olsen, 2007) ===
def mc_kl_from_samples(samples, logprob_p, logprob_q):
    """
    KL(P||Q) = E_{x~P}[ log p(x) - log q(x) ]
    """
    lp = logprob_p(samples)
    lq = logprob_q(samples)
    return float(np.mean(lp - lq))


@torch.no_grad()
def evaluate_kl_bounds(
    model,
    dataloader,
    device="cuda",
    n_batches=5,
    n_mdn_samples=500,
    seed=0,
    plot=True,
    save_plot=None,
    upper_kmax=6
):
    rng = np.random.default_rng(seed)

    # Pass 1: gather normalized queries for global Gaussian
    all_true, cache, taken = [], [], 0
    for (Xc, Xq, ep_stats, gmm_params) in dataloader:
        if taken >= n_batches:
            break
        cache.append((Xc, Xq, ep_stats, gmm_params))
        # no de-normalization here!
        all_true.extend([Xq[b].numpy() for b in range(Xq.shape[0])])
        taken += 1
    mu_glob, cov_glob = fit_global_gaussian(all_true)

    # Pass 2: per-episode KLs
    per_episode = []
    taken = 0
    for (Xc, Xq, ep_stats, gmm_params) in cache:
        Xc, Xq, ep_stats = [t.to(device) for t in (Xc, Xq, ep_stats)]
        B, Nc, D = Xc.shape
        Nq = Xq.shape[1]
        pi_logits, mu, sig = model(Xc, torch.zeros_like(Xq), ep_stats)

        for b in range(B):
            # --- Predicted MDN ---
            pi_b, mu_b, sig_b = (
                pi_logits[b].cpu().numpy(),
                mu[b].cpu().numpy(),
                sig[b].cpu().numpy(),
            )

            # Sample from MDN in normalized space
            comp = torch.distributions.Independent(
                torch.distributions.Normal(torch.tensor(mu_b, device=device),
                                           torch.tensor(sig_b, device=device)), 1
            )
            mix = torch.distributions.Categorical(logits=torch.tensor(pi_b, device=device))
            mdn = torch.distributions.MixtureSameFamily(mix, comp)

            samples = mdn.sample((n_mdn_samples,))  # (S,Nq,D), normalized
            samples = samples.cpu().numpy().reshape(-1, D)

            # logprob for MDN (normalized inputs)
            logprob_pred = lambda x: mdn_log_prob_numpy(pi_b, mu_b, sig_b, x)

            # --- True episodic GMM (normalized params from dataset) ---
            gmm_b = gmm_params[b]
            pis, means, sigmas = gmm_b["pis"].numpy(), gmm_b["means"].numpy(), gmm_b["sigmas"].numpy()
            logprob_true = lambda x: true_gmm_log_prob(pis, means, sigmas, x)

            kl_true = mc_kl_from_samples(samples, logprob_pred, logprob_true)

            # --- Upper bound baseline (fit GMM on normalized queries) ---
            Xq_np = Xq[b].cpu().numpy()
            gmm_up = fit_ep_gmm(Xq_np, k_max=upper_kmax, seed=seed)
            logprob_up = lambda x: gmm_up.score_samples(x)
            kl_upper = mc_kl_from_samples(samples, logprob_pred, logprob_up)

            # --- Lower bound baseline (global Gaussian, normalized space) ---
            mvn_low = torch.distributions.MultivariateNormal(
                torch.tensor(mu_glob, dtype=torch.float32, device=device),
                torch.tensor(cov_glob + 1e-6*np.eye(D), dtype=torch.float32, device=device),
            )
            logprob_low = lambda x: mvn_low.log_prob(
                torch.tensor(x, dtype=torch.float32, device=device)
            ).cpu().numpy()
            kl_lower = mc_kl_from_samples(samples, logprob_pred, logprob_low)

            # --- Store results ---
            per_episode.append({
                "kl_true": kl_true,
                "kl_upper": kl_upper,
                "kl_lower": kl_lower,
            })

        taken += 1
        if taken >= n_batches:
            break

    # === Aggregate results ===
    def agg(key):
        vals = np.array([row[key] for row in per_episode], dtype=float)
        return float(vals.mean()), float(vals.std())

    print("=== Per-episode KL (first 5) ===")
    for row in per_episode[:5]:
        print({k: round(float(v), 4) for k, v in row.items()})

    print("\n=== Aggregate (mean ± std) ===")
    for lab in ["true", "lower", "upper"]:
        m, s = agg(f"kl_{lab}")
        print(f"[{lab:6}] KL: {m:.4f} ± {s:.4f}")

    # === Plot results ===
    if plot:
        episodes = np.arange(len(per_episode))
        plt.figure(figsize=(10, 5))
        plt.plot(episodes, [row["kl_true"] for row in per_episode],
                 label="KL(Pred || True GMM)", marker="o")
        plt.plot(episodes, [row["kl_lower"] for row in per_episode],
                 label="KL(Pred || Global Gaussian)", marker="x")
        plt.plot(episodes, [row["kl_upper"] for row in per_episode],
                 label="KL(Pred || Ep-GMM, BIC)", marker="^")
        plt.xlabel("Episode index")
        plt.ylabel("KL Divergence")
        plt.legend()
        plt.grid(True, linestyle="--", alpha=0.6)
        plt.tight_layout()
        if save_plot:
            plt.savefig(save_plot, dpi=150)
            print(f"Saved KL plot to {save_plot}")
        else:
            plt.show()

    return per_episode

from torch.utils.data import DataLoader

def collate_with_gmmparams(batch):
    """
    Custom collate function for EpisodicGMM dataset.
    Keeps gmm_params as a list instead of stacking (since K varies per episode).
    """
    Xc, Xq, ep_stats, gmm_params = zip(*batch)
    return (
        torch.stack(Xc, dim=0),       # (B, Nc, D)
        torch.stack(Xq, dim=0),       # (B, Nq, D)
        torch.stack(ep_stats, dim=0), # (B, 2D)
        list(gmm_params),             # list of dicts, one per episode
    )


if __name__ == "__main__":
    # === Load dataset ===
    test_dataset = EpisodicGMM(
        n_episodes=200,   # number of test episodes
        dim=2,
        Nc=16,
        Nq=64,
        seed=42
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=8,
        shuffle=False,
        collate_fn=collate_with_gmmparams
    )

    # === Load trained model ===
    model = SetTransformerMDN(
        dim=2, d_model=128, nhead=8, nlayers=3,
        K_mdn=5, add_ep_stats=True
    ).to(DEVICE)

    model.load_state_dict(torch.load("icde_fmq_best.pt", map_location=DEVICE))
    model.eval()

    # === Run KL evaluation ===
    results = evaluate_kl_bounds(
        model=model,
        dataloader=test_loader,
        device=DEVICE,
        n_batches=10,             # number of batches to evaluate
        n_mdn_samples=500,        # MC samples per episode
        seed=0,
        plot=True,
        save_plot="kl_results.png"
    )

    print(f"\nEvaluated {len(results)} episodes")